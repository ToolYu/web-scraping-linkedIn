{"cells":[{"cell_type":"code","source":["!pip install --force-reinstall \"blinker<1.6\"\n","!pip install --upgrade selenium-wire"],"metadata":{"id":"NLZNn_APQHJw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737598984501,"user_tz":300,"elapsed":22324,"user":{"displayName":"Qianyu Zhang","userId":"03519155617719244523"}},"outputId":"b79c180d-43ff-4604-9da5-aab0a388859e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting blinker<1.6\n","  Downloading blinker-1.5-py2.py3-none-any.whl.metadata (1.8 kB)\n","Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n","Installing collected packages: blinker\n","  Attempting uninstall: blinker\n","    Found existing installation: blinker 1.9.0\n","    Uninstalling blinker-1.9.0:\n","      Successfully uninstalled blinker-1.9.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","flask 3.1.0 requires blinker>=1.9, but you have blinker 1.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed blinker-1.5\n","Collecting selenium-wire\n","  Downloading selenium_wire-5.1.0-py3-none-any.whl.metadata (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: blinker>=1.4 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (1.5)\n","Collecting brotli>=1.0.9 (from selenium-wire)\n","  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n","Requirement already satisfied: certifi>=2019.9.11 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (2024.12.14)\n","Collecting kaitaistruct>=0.7 (from selenium-wire)\n","  Downloading kaitaistruct-0.10-py2.py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pyasn1>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (0.6.1)\n","Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (24.2.1)\n","Requirement already satisfied: pyparsing>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (3.2.1)\n","Requirement already satisfied: pysocks>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (1.7.1)\n","Collecting selenium>=4.0.0 (from selenium-wire)\n","  Downloading selenium-4.28.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting wsproto>=0.14 (from selenium-wire)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Collecting zstandard>=0.14.1 (from selenium-wire)\n","  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Collecting h2>=4.0 (from selenium-wire)\n","  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n","Collecting hyperframe>=6.0 (from selenium-wire)\n","  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n","Collecting hpack<5,>=4.0 (from h2>=4.0->selenium-wire)\n","  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL>=22.0.0->selenium-wire) (43.0.3)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium>=4.0.0->selenium-wire) (2.3.0)\n","Collecting trio~=0.17 (from selenium>=4.0.0->selenium-wire)\n","  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting trio-websocket~=0.9 (from selenium>=4.0.0->selenium-wire)\n","  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.0.0->selenium-wire) (4.12.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.0.0->selenium-wire) (1.8.0)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->selenium-wire) (0.14.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44,>=41.0.5->pyOpenSSL>=22.0.0->selenium-wire) (1.17.1)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium>=4.0.0->selenium-wire) (24.3.0)\n","Collecting sortedcontainers (from trio~=0.17->selenium>=4.0.0->selenium-wire)\n","  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium>=4.0.0->selenium-wire) (3.10)\n","Collecting outcome (from trio~=0.17->selenium>=4.0.0->selenium-wire)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium>=4.0.0->selenium-wire) (1.3.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL>=22.0.0->selenium-wire) (2.22)\n","Downloading selenium_wire-5.1.0-py3-none-any.whl (239 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n","Downloading kaitaistruct-0.10-py2.py3-none-any.whl (7.0 kB)\n","Downloading selenium-4.28.0-py3-none-any.whl (9.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hpack-4.1.0-py3-none-any.whl (34 kB)\n","Downloading trio-0.28.0-py3-none-any.whl (486 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n","Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n","Installing collected packages: sortedcontainers, brotli, zstandard, wsproto, outcome, kaitaistruct, hyperframe, hpack, trio, h2, trio-websocket, selenium, selenium-wire\n","Successfully installed brotli-1.1.0 h2-4.1.0 hpack-4.1.0 hyperframe-6.1.0 kaitaistruct-0.10 outcome-1.3.0.post0 selenium-4.28.0 selenium-wire-5.1.0 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 wsproto-1.2.0 zstandard-0.23.0\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"v8wH2IOUL_KJ","executionInfo":{"status":"ok","timestamp":1737599228142,"user_tz":300,"elapsed":6097,"user":{"displayName":"Qianyu Zhang","userId":"03519155617719244523"}},"colab":{"base_uri":"https://localhost:8080/","height":77},"outputId":"4d317b60-9d9d-49f7-9d51-1824bf414e4a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-66dfa34c-8ce5-47a4-9daf-44c3f70ec102\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-66dfa34c-8ce5-47a4-9daf-44c3f70ec102\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving cookies.txt to cookies.txt\n"]}]},{"cell_type":"code","source":["# !rm -rf *\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZl8x0iMYWPU","executionInfo":{"status":"ok","timestamp":1737599229188,"user_tz":300,"elapsed":144,"user":{"displayName":"Qianyu Zhang","userId":"03519155617719244523"}},"outputId":"1fc31f57-7fe2-4e3b-f6c0-d23e28b7ddc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cookies.txt  sample_data\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ub_9YjvqzNsq","colab":{"base_uri":"https://localhost:8080/","height":252,"referenced_widgets":["1b745fb201e5443d89b75298b5e1998f","0e68af33ea654acbbf93ef0c221d1db0","a0c7c8bb101741ca95fc6895e96a164f","2dd5a47a8368408b88f4b10bce32a49e","488d04753a8b4dd8851104816a441f2e","ff4cc506abf34cc09b38efbed15080cd","92b66899b5804ba1bd303f25907fe3ce","8f60c8b0b99744a68e94305a8a2a2025","7cf919bf66594863a16d4be0b5ef5cf1","41b0aa265b714a9daa87e67f77fa3be2","b40548d6258c4736b01809202d99fb66"]},"executionInfo":{"status":"ok","timestamp":1737599234160,"user_tz":300,"elapsed":2230,"user":{"displayName":"Qianyu Zhang","userId":"03519155617719244523"}},"outputId":"2bbe534b-32b3-4ebe-8722-1df66a122b90"},"outputs":[{"output_type":"display_data","data":{"text/plain":["BoundedIntText(value=10, description='Number of Posts:', max=1000, min=1)"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b745fb201e5443d89b75298b5e1998f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Textarea(value='https://www.linkedin.com/company/knowledge-wharton/posts/?feedView=all', description='Profile …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd5a47a8368408b88f4b10bce32a49e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(button_style='success', description='Start Scraping', icon='check', style=ButtonStyle(), tooltip='Click…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92b66899b5804ba1bd303f25907fe3ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41b0aa265b714a9daa87e67f77fa3be2"}},"metadata":{}}],"source":["# Import necessary libraries\n","from seleniumwire import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.common.exceptions import TimeoutException, NoSuchElementException\n","import time\n","from bs4 import BeautifulSoup as bs\n","import csv\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","import os\n","import requests\n","from urllib.parse import urlparse\n","import hashlib\n","import json\n","import datetime\n","import subprocess\n","\n","# Function to load cookies from a Netscape format cookies.txt file\n","\n","def load_cookies(browser, file_path):\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            if not line.startswith('#') and line.strip():\n","                fields = line.strip().split('\\t')\n","                if len(fields) == 7:\n","                    cookie = {\n","                        'domain': fields[0],\n","                        'flag': fields[1],\n","                        'path': fields[2],\n","                        'secure': fields[3],\n","                        'expiration': fields[4],\n","                        'name': fields[5],\n","                        'value': fields[6]\n","                    }\n","                    # Adjust domain if it starts with a dot\n","                    domain = cookie['domain'].lstrip('.')\n","                    browser.add_cookie({\n","                        'name': cookie['name'],\n","                        'value': cookie['value'],\n","                        'domain': domain,\n","                        'path': cookie['path'],\n","                        'expiry': int(cookie['expiration']) if cookie['expiration'] else None\n","                    })\n","\n","# Function to get cookies from Selenium Wire browser and convert them to requests-compatible format\n","def get_requests_cookies(browser):\n","    selenium_cookies = browser.get_cookies()\n","    requests_cookies = {}\n","    for cookie in selenium_cookies:\n","        requests_cookies[cookie['name']] = cookie['value']\n","    return requests_cookies\n","\n","# Function to extract images with \"media\" in their src, excluding those with 'company-logo'\n","def extract_images(post_content_container):\n","    post_images = []\n","    try:\n","        post_media_container = post_content_container.find_next(\"div\", {\"class\": \"feed-shared-actor__container\"}) if post_content_container else None\n","        if post_media_container:\n","            # Extract images\n","            images = post_media_container.find_all(\"img\", {\"class\": \"feed-shared-image__image\"})\n","            for img in images:\n","                img_url = img.get('src') or img.get('data-delayed-url')\n","                if img_url and 'media' in img_url.lower() and 'company-logo' not in img_url.lower():\n","                    post_images.append(img_url)\n","    except Exception as e:\n","        print(f\"Error extracting media images: {e}\")\n","\n","    # Alternative extraction\n","    if not post_images:\n","        try:\n","            post_container = post_content_container.find_parent(\"div\", {\"class\": \"occludable-update\"}) if post_content_container else None\n","            if post_container:\n","                images = post_container.find_all(\"img\")\n","                for img in images:\n","                    img_url = img.get('src') or img.get('data-delayed-url')\n","                    if img_url and 'media' in img_url.lower() and 'company-logo' not in img_url.lower() and 'data:image' not in img_url:\n","                        post_images.append(img_url)\n","        except Exception as e:\n","            print(f\"Alternative media image extraction failed: {e}\")\n","\n","    # Remove duplicates and return\n","    return list(set(post_images))\n","\n","# Function to download images\n","def download_images(image_urls, folder_path, post_number):\n","    if not os.path.exists(folder_path):\n","        os.makedirs(folder_path)\n","\n","    for idx, img_url in enumerate(image_urls, start=1):\n","        try:\n","            response = requests.get(img_url, timeout=10)\n","            response.raise_for_status()  # Raise an exception for HTTP errors\n","            # Extract image extension from URL\n","            parsed_url = urlparse(img_url)\n","            _, ext = os.path.splitext(parsed_url.path)\n","            if not ext:\n","                ext = '.jpg'  # Default extension if none found\n","            image_name = f\"image_{idx}{ext}\"\n","            image_path = os.path.join(folder_path, image_name)\n","            with open(image_path, 'wb') as img_file:\n","                img_file.write(response.content)\n","            print(f\"Downloaded {image_name} for Post {post_number}\")\n","        except requests.exceptions.RequestException as e:\n","            print(f\"Failed to download image from {img_url}: {e}\")\n","\n","# Function to extract video URLs via network requests\n","def extract_m3u8_via_network(requests):\n","    \"\"\"\n","    Looks for an HLS (.m3u8) playlist in the network requests.\n","    Returns the first .m3u8 URL found, or None if none is found.\n","    \"\"\"\n","    for request in requests:\n","        if request.response:\n","            content_type = request.response.headers.get('Content-Type', '').lower()\n","            if 'application/vnd.apple.mpegurl' in content_type or 'application/x-mpegurl' in content_type:\n","                # We found an HLS playlist\n","                return request.url\n","    return None\n","def download_hls_video(m3u8_url, folder_path, video_name, cookies_file, browser):\n","    \"\"\"\n","    Downloads an HLS video from the given m3u8_url using ffmpeg,\n","    saving it into folder_path as video_name.\n","    Includes cookies/headers for authentication, loaded from cookies.txt.\n","    \"\"\"\n","    # Load cookies from cookies.txt\n","    load_cookies(browser, cookies_file)  # Ensure browser is passed in\n","\n","    # Get cookies directly from browser\n","    cookies = browser.get_cookies()  # Get cookies from the Selenium browser instance\n","\n","    if not cookies:\n","        print(\"No cookies found. Exiting...\")\n","        return\n","\n","    # Create a cookie string for the ffmpeg command\n","    cookie_string = '; '.join([f'{cookie[\"name\"]}={cookie[\"value\"]}' for cookie in cookies])\n","\n","    # Prepare the custom HTTP headers\n","    cookie_header = f\"Cookie: {cookie_string}\"\n","    referer_header = \"Referer: https://www.linkedin.com/\"\n","\n","    # Set the output path for the video\n","    output_path = os.path.join(folder_path, video_name)\n","\n","    # ffmpeg command to download the video\n","    command = [\n","        \"ffmpeg\",\n","        \"-y\",  # Overwrite output file if it exists\n","        \"-headers\", cookie_header,  # Add cookies as headers\n","        \"-headers\", referer_header,  # Add Referer header\n","        \"-i\", m3u8_url,  # Input URL\n","        \"-c\", \"copy\",  # Copy original video/audio without re-encoding\n","        output_path  # Output file path\n","    ]\n","\n","    print(f\"\\nAttempting to download HLS video to {output_path}\")\n","    try:\n","        subprocess.run(command, check=True)\n","        print(f\"Successfully downloaded HLS video as {output_path}\")\n","    except subprocess.CalledProcessError as e:\n","        print(f\"ffmpeg error: {e}\")\n","    except Exception as e:\n","        print(f\"Error downloading HLS video: {e}\")\n","\n","\n","def scrape_linkedin_posts(max_posts, user_profile_url):\n","    chrome_options = Options()\n","    chrome_options.add_argument('--headless')  # Run in headless mode (optional)\n","    chrome_options.add_argument('--no-sandbox')\n","    chrome_options.add_argument('--disable-dev-shm-usage')\n","\n","    # ## Load the Chrome extension\n","    # extension_path = \"/Users/qianyuzhang/Library/Application Support/Google/Chrome/Default/Extensions/hkgafbpgfpjjamcgkfdkdocppfpcjjhn/1.4_0\"\n","    # chrome_options.add_argument(f'--load-extension={extension_path}')\n","\n","    print(\"Initializing the Chrome driver with Selenium Wire...\")\n","    browser = webdriver.Chrome(options=chrome_options)\n","\n","    # Verify that 'requests' attribute exists\n","    if not hasattr(browser, 'requests'):\n","        print(\"Error: 'requests' attribute not found in the WebDriver. Ensure Selenium Wire is correctly installed and imported.\")\n","        browser.quit()\n","        return\n","\n","    # Initialize a set to keep track of processed post contents\n","    processed_contents = set()\n","\n","    # Tracking: Load processed_contents from a file to persist across runs\n","    processed_file = 'processed_posts.json'\n","    if os.path.exists(processed_file):\n","        with open(processed_file, 'r') as f:\n","            try:\n","                processed_contents = set(json.load(f))\n","                print(f\"Loaded {len(processed_contents)} previously processed posts.\")\n","            except json.JSONDecodeError:\n","                print(\"Processed posts file is empty or corrupted. Starting fresh.\")\n","                processed_contents = set()\n","\n","\n","    try:\n","        # Set the window size\n","        browser.set_window_size(1920, 1080)\n","\n","        # Open LinkedIn login page\n","        print(\"Opening LinkedIn login page...\")\n","        browser.get('https://www.linkedin.com/')\n","\n","        # Wait for the page to load\n","        time.sleep(5)\n","\n","        # Load cookies from the file\n","        print(\"Loading cookies...\")\n","        load_cookies(browser, 'cookies.txt')  # Ensure 'cookies.txt' is in the same directory\n","        print(\"Cookies loaded.\")\n","\n","        # Refresh the page to apply cookies\n","        browser.refresh()\n","\n","        # Wait for the main navigation bar to be visible\n","        print(\"Waiting for the main navigation bar after applying cookies...\")\n","        try:\n","            WebDriverWait(browser, 30).until(\n","                EC.presence_of_element_located((By.CSS_SELECTOR, '#global-nav .global-nav__me'))\n","            )\n","            print(\"Navigation bar found. Proceeding with scraping...\")\n","        except TimeoutException:\n","            print(\"TimeoutException: Navigation bar not found after applying cookies. Check if the cookies are correct.\")\n","            return\n","\n","        # Navigate to the user's recent activity page\n","        print(f\"Navigating to the user's recent activity page: {user_profile_url}...\")\n","        browser.get(user_profile_url)\n","\n","        # Wait for the page to load completely\n","        print(\"Waiting for the user's recent activity page to load completely...\")\n","        time.sleep(5)\n","\n","        # Generate a unique run identifier based on the current datetime\n","        run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        parent_folder = f\"Scrape_{run_id}\"\n","        os.makedirs(parent_folder, exist_ok=True)\n","        print(f\"Created parent folder: {parent_folder}\")\n","\n","        # Set parameters for scrolling through the page\n","        SCROLL_PAUSE_TIME = 2  # Time to pause between scrolls\n","        LOAD_PAUSE_TIME = 10   # Time to wait after scrolling to load new posts\n","\n","        # Initialize a list to store posts data\n","        posts_data = []\n","        post_count = 0\n","\n","        # Create CSV — Notice we added 'Post Date' below\n","        csv_file = f\"user_posts_{run_id}.csv\"\n","        with open(csv_file, mode='w', encoding='utf-8', newline='') as file:\n","            writer = csv.writer(file)\n","            writer.writerow([\n","                'Post Number',\n","                'Post Date',       # <-- New column for date\n","                'Content',\n","                'Reactions',\n","                'Comments',\n","                'Media Type',\n","                'Image URL',\n","                'Video URL',\n","                'Article URL'\n","            ])\n","\n","        while post_count < max_posts:\n","            # Parse the page source with BeautifulSoup\n","            user_page = browser.page_source\n","            linkedin_soup = bs(user_page.encode(\"utf-8\"), \"html.parser\")\n","\n","            # Extract post containers from the HTML\n","            containers = linkedin_soup.find_all(\"div\", {\"class\": \"occludable-update\"})\n","\n","            # Clear previous network requests\n","            browser.requests.clear()\n","\n","            # Main loop to process each container\n","            print(\"Processing each container to extract post data...\")\n","            for container in containers:\n","                if post_count >= max_posts:\n","                    break\n","\n","                # -----------------------------------\n","                # 1) Extract Post Date\n","                #    Adjust your CSS selectors if needed based on LinkedIn DOM updates.\n","                # -----------------------------------\n","                try:\n","                    # One known pattern is a span with class \"feed-shared-actor__sub-description...\"\n","                    # but this can change. Adjust as necessary.\n","                    date_container = container.find(\n","                        \"span\",\n","                        class_=\"update-components-actor__sub-description t-12 t-normal t-black--light\"\n","\n","                    )\n","\n","                    post_date = date_container.get_text(separator=' ').strip() if date_container else \"No date\"\n","                except Exception as e:\n","                    print(f\"Error extracting post date: {e}\")\n","                    post_date = \"No date\"\n","\n","                try:\n","                    # 2) Extract post content\n","                    post_content_container = container.find(\"div\", {\"class\": \"update-components-text\"})\n","                    post_content = post_content_container.get_text(separator=' ').strip() if post_content_container else \"No content\"\n","\n","                    # Check if the post has no content and skip if so\n","                    if post_content == \"No content\":\n","                        print(\"Post has no content. Skipping.\")\n","                        continue\n","\n","                    # Generate a hash of the post content for duplication detection\n","                    post_hash = hashlib.md5(post_content.encode('utf-8')).hexdigest()\n","\n","                    if post_hash in processed_contents:\n","                        print(\"Duplicate post found based on content. Skipping.\")\n","                        continue  # Skip duplicate posts\n","\n","                    # Add the hash to the set to mark it as processed\n","                    processed_contents.add(post_hash)\n","\n","                except Exception as e:\n","                    print(f\"Error extracting post content: {e}\")\n","                    post_content = \"No content\"\n","                    print(\"Post has no content due to an error. Skipping.\")\n","                    continue\n","\n","                # Extract Reactions\n","                try:\n","                    reactions_button = container.find(\"li\", {\"class\": \"social-details-social-counts__reactions\"}).find(\"button\")\n","                    post_reactions = reactions_button[\"aria-label\"].split(\" \")[0].replace(',', '') if reactions_button else \"0\"\n","                except:\n","                    post_reactions = \"0\"\n","\n","                # Extract Comments\n","                try:\n","                    comments_button = container.find(\"li\", {\"class\": \"social-details-social-counts__comments\"}).find(\"button\")\n","                    post_comments = comments_button[\"aria-label\"].split(\" \")[0].replace(',', '') if comments_button else \"0\"\n","                except:\n","                    post_comments = \"0\"\n","\n","                # Initialize media content lists\n","                post_images = []\n","                post_videos = []\n","                post_links = []\n","\n","                # Extract images\n","                if post_content_container:\n","                    post_images = extract_images(post_content_container)\n","\n","                # Extract links\n","                try:\n","                    if post_content_container:\n","                        links = post_content_container.find_all(\"a\", href=True)\n","                        for link in links:\n","                            href = link['href']\n","                            if href.startswith('http'):  # Ensure it's an absolute URL\n","                                post_links.append(href)\n","                except Exception as e:\n","                    print(f\"Error extracting links: {e}\")\n","\n","                # Remove duplicates\n","                post_images = list(set(post_images))\n","                post_links = list(set(post_links))\n","\n","                # Determine Media Type (default = 'Text')\n","                media_type = 'Text'\n","                videos_str = \"No videos\"\n","                images_str = \"No images\" if not post_images else '; '.join(post_images)\n","                links_str = \"No links\" if not post_links else '; '.join(post_links)\n","\n","                # Increment post count\n","                post_count += 1\n","                post_number = post_count\n","\n","                # Create a folder for the current post\n","                folder_name = f\"Post_{post_number}\"\n","                folder_path = os.path.join(parent_folder, folder_name)\n","                os.makedirs(folder_path, exist_ok=True)\n","\n","                # Download images if any\n","                if post_images:\n","                    download_images(post_images, folder_path, post_number)\n","\n","                # Scroll to the post to ensure potential videos are loaded\n","                try:\n","                    selenium_post_element = browser.find_element(\n","                        By.XPATH, f\"(//div[contains(@class, 'occludable-update')])[{post_number}]\"\n","                    )\n","                    browser.execute_script(\"arguments[0].scrollIntoView();\", selenium_post_element)\n","                    time.sleep(2)  # Wait for dynamic content\n","                except Exception as e:\n","                    print(f\"Error scrolling to post {post_number}: {e}\")\n","\n","                has_video_element = bool(container.find(\"video\"))\n","\n","                # Clear previous requests again, just to capture fresh requests for the current post\n","                browser.requests.clear()\n","\n","                if has_video_element:\n","                    print(\"Video element detected in the DOM. Waiting for network requests...\")\n","                    # Wait a bit to capture video requests\n","                    time.sleep(LOAD_PAUSE_TIME)\n","\n","                    # Extract video URLs from network requests\n","                    m3u8_url = extract_m3u8_via_network(browser.requests)\n","                    if m3u8_url:\n","                        print(f\"Found M3U8 link for Post {post_number}: {m3u8_url}\")\n","                        # We'll treat it as a video post\n","                        media_type = 'Video'\n","                        videos_str = m3u8_url\n","                        # Download using ffmpeg\n","                        download_hls_video(\n","                            m3u8_url=m3u8_url,\n","                            folder_path=folder_path,\n","                            video_name=\"video_hls.mp4\",\n","                            cookies_file=\"cookies.txt\",  # Pass the path to your cookies.txt here\n","                            browser=browser  # Pass browser here to access cookies\n","                        )\n","                    else:\n","                        print(f\"No valid MP4 or M3U8 link found for Post {post_number}.\")\n","                        media_type = \"Text\"\n","                        videos_str = \"No videos\"\n","                else:\n","                    print(f\"No <video> element found for Post {post_number}. Skipping video extraction.\")\n","\n","                # If there's no video, check if it might be an article link or image\n","                if media_type != 'Video':\n","                    if any('articleshare-shrink' in img_url for img_url in post_images):\n","                        media_type = 'Article Link'\n","                    elif post_images:\n","                        media_type = 'Image'\n","                    else:\n","                        media_type = 'Text'\n","\n","                # Append post data\n","                post_data = {\n","                    'Post Number': post_number,\n","                    'Post Date': post_date,  # <-- Include the extracted date\n","                    'Content': post_content,\n","                    'Reactions': post_reactions,\n","                    'Comments': post_comments,\n","                    'Media Type': media_type,\n","                    'Images URL': images_str,\n","                    'Videos URL': videos_str,\n","                    'Links': links_str,\n","                }\n","                posts_data.append(post_data)\n","\n","                print(f\"Post {post_count} processed and data saved.\")\n","\n","                # Save the post data to the CSV file immediately\n","                with open(csv_file, mode='a', encoding='utf-8', newline='') as file:\n","                    writer = csv.writer(file)\n","                    writer.writerow([\n","                        post_number,\n","                        post_date,\n","                        post_content,\n","                        post_reactions,\n","                        post_comments,\n","                        media_type,\n","                        images_str,\n","                        videos_str,\n","                        links_str\n","                    ])\n","\n","                # Tracking: Save the updated processed_contents to persist across runs\n","                with open(processed_file, 'w') as f:\n","                    json.dump(list(processed_contents), f)\n","\n","            if post_count < max_posts:\n","                # Scroll down to load more posts\n","                print(\"Scrolling down to load more posts...\")\n","                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","                time.sleep(LOAD_PAUSE_TIME)\n","            else:\n","                break\n","\n","        print(\"Finished scrolling and processing posts.\")\n","        print(f\"Data exported to {csv_file}\")\n","\n","    finally:\n","        # Close the browser\n","        browser.quit()\n","\n","\n","# Function to handle user input and initiate scraping\n","def start_scraping(button):\n","    with output:\n","        clear_output()\n","        try:\n","            max_posts = int(post_count_widget.value)\n","            user_profile_url = profile_url_widget.value.strip()\n","            if not user_profile_url:\n","                print(\"Please enter a valid LinkedIn profile URL.\")\n","                return\n","            print(f\"Starting scraping for {max_posts} posts from {user_profile_url}...\")\n","            scrape_linkedin_posts(max_posts, user_profile_url)\n","        except ValueError:\n","            print(\"Please enter a valid number for the number of posts.\")\n","\n","# Create interactive widgets\n","post_count_widget = widgets.BoundedIntText(\n","    value=10,\n","    min=1,\n","    max=1000,\n","    step=1,\n","    description='Number of Posts:',\n","    disabled=False\n",")\n","\n","# **Updated Widget: Using Textarea instead of Text for URL Input**\n","profile_url_widget = widgets.Textarea(\n","    value='https://www.linkedin.com/company/knowledge-wharton/posts/?feedView=all',\n","    placeholder='Enter LinkedIn Profile URL',\n","    description='Profile URL:',\n","    disabled=False,\n","    layout=widgets.Layout(width='100%', height='60px')  # Adjust size as needed\n",")\n","\n","submit_button = widgets.Button(\n","    description='Start Scraping',\n","    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n","    tooltip='Click to start scraping',\n","    icon='check'\n",")\n","\n","output = widgets.Output()\n","\n","submit_button.on_click(start_scraping)\n","\n","# Display the widgets\n","display(post_count_widget, profile_url_widget, submit_button, output)\n"]},{"cell_type":"code","source":["\n"],"metadata":{"id":"8Wx0eTwm9ZGT"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"stern","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"1b745fb201e5443d89b75298b5e1998f":{"model_module":"@jupyter-widgets/controls","model_name":"BoundedIntTextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"BoundedIntTextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntTextView","continuous_update":false,"description":"Number of Posts:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_0e68af33ea654acbbf93ef0c221d1db0","max":1000,"min":1,"step":1,"style":"IPY_MODEL_a0c7c8bb101741ca95fc6895e96a164f","value":400}},"0e68af33ea654acbbf93ef0c221d1db0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0c7c8bb101741ca95fc6895e96a164f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dd5a47a8368408b88f4b10bce32a49e":{"model_module":"@jupyter-widgets/controls","model_name":"TextareaModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextareaModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextareaView","continuous_update":true,"description":"Profile URL:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_488d04753a8b4dd8851104816a441f2e","placeholder":"Enter LinkedIn Profile URL","rows":null,"style":"IPY_MODEL_ff4cc506abf34cc09b38efbed15080cd","value":"https://www.linkedin.com/company/knowledge-wharton/posts/?feedView=all"}},"488d04753a8b4dd8851104816a441f2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"60px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"ff4cc506abf34cc09b38efbed15080cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92b66899b5804ba1bd303f25907fe3ce":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"success","description":"Start Scraping","disabled":false,"icon":"check","layout":"IPY_MODEL_8f60c8b0b99744a68e94305a8a2a2025","style":"IPY_MODEL_7cf919bf66594863a16d4be0b5ef5cf1","tooltip":"Click to start scraping"}},"8f60c8b0b99744a68e94305a8a2a2025":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cf919bf66594863a16d4be0b5ef5cf1":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"41b0aa265b714a9daa87e67f77fa3be2":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_b40548d6258c4736b01809202d99fb66","msg_id":"4960379e-3d81-4f93-fc3b-905fc0c3cb96","outputs":[{"output_type":"stream","name":"stdout","text":["Starting scraping for 400 posts from https://www.linkedin.com/company/knowledge-wharton/posts/?feedView=all...\n","Initializing the Chrome driver with Selenium Wire...\n"]},{"output_type":"stream","name":"stdout","text":["Opening LinkedIn login page...\n"]},{"output_type":"stream","name":"stdout","text":["Loading cookies...\n"]},{"output_type":"stream","name":"stdout","text":["Cookies loaded.\n"]}]}},"b40548d6258c4736b01809202d99fb66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}